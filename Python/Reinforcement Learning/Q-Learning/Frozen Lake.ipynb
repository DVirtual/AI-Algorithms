{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning (Frozen Lake)\n",
    "Implementing Markov Decision Process (MDP) in OpenAI-gym's environment, \"FrozenLake-v0\".\n",
    "MDP is defined as:\n",
    "$$Q_{(s,a)} = Q_{(s,a)} + \\alpha[R_{(s,a)} + \\gamma(\\arg \\max_{a'} \\mathbb{E}(Q_{(s',a')})) - Q_{(s,a)}]$$\n",
    "where $\\mathbb{E}$ is the expected return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v0\")\n",
    "lr = 0.8 # learning rate\n",
    "gamma = 0.95 # discount factor\n",
    "num_episodes = 10000 # total number of episodes\n",
    "\n",
    "epsilon = 1.0 # exploration rate\n",
    "max_epsilon = 1.0 \n",
    "min_epsilon = 0.01\n",
    "decay_rate = 0.005 # Exponential decay for exploration probability\n",
    "\n",
    "max_steps = 99 # maximum number of steps in an episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_size = env.observation_space.n # state space size\n",
    "act_size = env.action_space.n # action space size\n",
    "q_table = np.zeros(shape=(obs_size, act_size)) # Q(s,a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Rewards: 0.481\n",
      "[[4.94516775e-02 5.21610074e-02 4.96585496e-02 5.20754369e-02]\n",
      " [4.57377757e-03 2.82504374e-03 2.43040769e-02 4.62734814e-02]\n",
      " [4.00933728e-03 1.49580058e-02 6.24570049e-03 2.88932233e-02]\n",
      " [3.33051576e-03 1.10577548e-03 3.44539687e-03 2.05349727e-02]\n",
      " [6.20192924e-02 1.05187084e-02 4.45745640e-02 2.47123410e-04]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [6.23470159e-07 4.47087661e-07 1.03648060e-03 8.27040581e-08]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [5.77972891e-02 8.02003742e-03 4.76025794e-02 6.61519625e-02]\n",
      " [1.10402905e-02 3.68646240e-02 2.19531992e-02 3.78872579e-02]\n",
      " [1.00050182e-02 1.94214427e-02 9.70403755e-04 9.00712851e-04]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [1.19268049e-01 1.51767195e-01 3.62854790e-01 6.96852929e-02]\n",
      " [2.05442859e-01 3.85004416e-01 1.24462073e-01 8.95022059e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "total_rewards = []\n",
    "for i in range(num_episodes):\n",
    "    done = False\n",
    "    s = env.reset() # resetting the environment\n",
    "    rewards = 0\n",
    "    \n",
    "    for j in range(max_steps):\n",
    "        if np.random.uniform() < epsilon:\n",
    "            a = env.action_space.sample()\n",
    "        else:\n",
    "            a = np.argmax(q_table[s,:])\n",
    "        \n",
    "        s1, r, done, _ = env.step(a)\n",
    "        rewards += r\n",
    "        \n",
    "        # Updating Q(s,a) := Q(s,a) + lr * (r + gamme * max Q(s',a') - Q(s,a))\n",
    "        q_table[s,a] = q_table[s,a] + lr * (r + gamma * np.max(q_table[s1,:]) - q_table[s,a])\n",
    "        s = s1\n",
    "        if done:\n",
    "            break\n",
    "    total_rewards.append(rewards)\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*i)\n",
    "\n",
    "print(f\"Mean Rewards: {np.mean(total_rewards)}\")\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
